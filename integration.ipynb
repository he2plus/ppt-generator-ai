{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBIm-DnyusLt"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate sacremoses einops xformers\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers"
      ],
      "metadata": {
        "id": "2YGSStJN1mK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GPUtil"
      ],
      "metadata": {
        "id": "VgO2svp01ldl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Falcon LLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch"
      ],
      "metadata": {
        "id": "lfxpzvBwvK0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stable Diffusion\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import os"
      ],
      "metadata": {
        "id": "RdjWxh9rvUA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU Utility Checking\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "import gc"
      ],
      "metadata": {
        "id": "uBlfyNzb12uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gpu_usage())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7NiDLHK2GfP",
        "outputId": "dbf9ec0d-3705-4823-c029-9155b43c02e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% |  0% |\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Falcon LLM\n",
        "model = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "BIeik3uavX0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Falcon LLM\n",
        "sequences = pipeline(\n",
        "    \"Generate a 3 slides presentation on 'indus valley civilization' with headings and content\",\n",
        "    # batch_size=64,\n",
        "    num_workers=8,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id = tokenizer.eos_token_id,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "pwdlU1K3vw4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62610f27-cc04-4443-8781-0c8493ef9379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: Generate a 3 slides presentation on 'indus valley civilization' with headings and content\n",
            "Slide 1: Introduction:\n",
            "Indus Valley Civilization was one of the earliest civilizations in the world. It is mainly associated with the Indo-Iranian cultural and historical region. It is also commonly referred to as the Indus Valley Civilization. It is considered as one of the most complex and advanced civilizations of ancient and pre-modern times.\n",
            "\n",
            "Slide 2: Architecture and Art:\n",
            "The Indus Valley Civilization was well-known for its remarkable architectural and artistic achievements. It is said that the ancient Indus Valley was well-planned and organized. The cities were laid out in accordance to a specific system, and each structure had a specific use and purpose. The architecture of the Indus Valley was unique, featuring the use of brick and stone as building materials.\n",
            "\n",
            "Slide 3: Agriculture, Economy, and Social Structure:\n",
            "Indus Valley Civilization had remarkable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del tokenizer\n",
        "del pipeline"
      ],
      "metadata": {
        "id": "vYxjNQBD2Ppd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gpu_usage())\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(gpu_usage())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePMlOGPb2FEG",
        "outputId": "41b7ef3c-2d0f-4f94-9c23-ffeb830f5544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 | 99% | 96% |\n",
            "None\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% | 95% |\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stable Diffusion\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
        "pipe = pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "RuuoA-oXvjZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stable Diffusion\n",
        "path = \"./\"\n",
        "\n",
        "prompt = \"a man riding horse on moon\"\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "image_name = prompt.lower().replace(\" \", \"_\")\n",
        "\n",
        "image_name = list(image_name)\n",
        "a=['.','p','n','g']\n",
        "for i in range(0,4):\n",
        "  image_name.append(a[i])\n",
        "image_name=''.join(image_name)\n",
        "\n",
        "pathe = \"\"\n",
        "image_name = list(image_name)\n",
        "path = list(path)\n",
        "for i in range(0,len(image_name)):\n",
        "  path.append(image_name[i])\n",
        "pathe=''.join(path)\n",
        "\n",
        "image.save(pathe)"
      ],
      "metadata": {
        "id": "fXiHtz5kv7mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model_id\n",
        "del pipe"
      ],
      "metadata": {
        "id": "2drl_Z_c2YNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gpu_usage())\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(gpu_usage())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXgRbfM-2X8P",
        "outputId": "a4814499-f2d3-497f-9890-585abae4adbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 | 78% | 97% |\n",
            "None\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% | 46% |\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}